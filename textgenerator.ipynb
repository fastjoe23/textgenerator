{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9640a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a144c990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1527063 characters\n"
     ]
    }
   ],
   "source": [
    "#path_to_file = \".\\\\data\\\\tinyshakespeare\\\\input.txt\"\n",
    "path_to_file = \".\\\\data\\\\amazon_reviews\\\\Sports_and_Outdoors_reviewTexts_min.txt\"\n",
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af783c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 unique characters\n",
      "['\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~']\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9e436b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, '\\r': 1, ' ': 2, '!': 3, '\"': 4, '#': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, '*': 12, '+': 13, ',': 14, '-': 15, '.': 16, '/': 17, '0': 18, '1': 19, '2': 20, '3': 21, '4': 22, '5': 23, '6': 24, '7': 25, '8': 26, '9': 27, ':': 28, ';': 29, '=': 30, '>': 31, '?': 32, '@': 33, 'A': 34, 'B': 35, 'C': 36, 'D': 37, 'E': 38, 'F': 39, 'G': 40, 'H': 41, 'I': 42, 'J': 43, 'K': 44, 'L': 45, 'M': 46, 'N': 47, 'O': 48, 'P': 49, 'Q': 50, 'R': 51, 'S': 52, 'T': 53, 'U': 54, 'V': 55, 'W': 56, 'X': 57, 'Y': 58, 'Z': 59, '[': 60, ']': 61, '_': 62, '`': 63, 'a': 64, 'b': 65, 'c': 66, 'd': 67, 'e': 68, 'f': 69, 'g': 70, 'h': 71, 'i': 72, 'j': 73, 'k': 74, 'l': 75, 'm': 76, 'n': 77, 'o': 78, 'p': 79, 'q': 80, 'r': 81, 's': 82, 't': 83, 'u': 84, 'v': 85, 'w': 86, 'x': 87, 'y': 88, 'z': 89, '~': 90}\n",
      "[53 71 72 ... 68  1  0]\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "print(char2idx)\n",
    "print(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "728082f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4733ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "i\n",
      "s\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# First 5 Letters of InputText\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9455cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ba55e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This came in on time and I am veru happy with it, I haved used it already and it makes taking out the'\n",
      "' pins in my glock 32 very easy\\r\\nI had a factory Glock tool that I was using for my Glock 26, 27, and '\n",
      "\"17.  I've since lost it and had needed another.  Since I've used Ghost products prior, and know that \"\n",
      "'they are reliable, I had decided to order this one.  Sure enough, this is just as good as a factory t'\n",
      "\"ool.\\r\\nIf you don't have a 3/32 punch or would like to have one in your Glock bag, this is okay.  The \"\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32fbe77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53cdb644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58404a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'This came in on time and I am veru happy with it, I haved used it already and it makes taking out th'\n",
      "Target data: 'his came in on time and I am veru happy with it, I haved used it already and it makes taking out the'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2df8c0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "deb5ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5502872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "afb8f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddfe3d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 91) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7766a5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (64, None, 256)           23296     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (64, None, 91)            93275     \n",
      "=================================================================\n",
      "Total params: 4,054,875\n",
      "Trainable params: 4,054,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d982cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 91)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.5117354\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3afb90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333d3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " 86/236 [=========>....................] - ETA: 3:19 - loss: 2.9991"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 4\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "history = model.fit(dataset, epochs=number_of_epochs, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d8fafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b987a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 200\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  \n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "610075ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This have to to have and a sume a promecting compend it it is a bat hore have in the beat to the porke.  I hand expacher a smake the recass cood not a coul the this and tre sare bo bas a ont and a with the\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"This \", temperature=.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c585ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models\\model_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models\\model_1\\assets\n"
     ]
    }
   ],
   "source": [
    "# Directory where the models will be saved\n",
    "model_dir = './saved_models'\n",
    "# Name of the model files\n",
    "model_prefix = os.path.join(model_dir, \"model_\" + str(number_of_epochs))\n",
    "model.save(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd11460a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            23296     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 91)             93275     \n",
      "=================================================================\n",
      "Total params: 4,054,875\n",
      "Trainable params: 4,054,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(model_prefix)\n",
    "\n",
    "# Check model architecture\n",
    "loaded_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "729427a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This my shat the the a for the prooled it for is loen to some of the scomple so mont thas and the tou cheal semp a out to ase the thit seal your on the prowice.  I seme and it some it small to a stapp and \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"This \", temperature=.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
